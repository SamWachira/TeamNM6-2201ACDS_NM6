{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "import random; random.seed(53)\n",
    "\n",
    "# Import all we need from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    PolySciMajor EPA chief doesn't think carbon di...\n",
       "1    It's not like we lack evidence of anthropogeni...\n",
       "2    RT @RawStory: Researchers say we have three ye...\n",
       "3    #TodayinMaker# WIRED : 2016 was a pivotal year...\n",
       "4    RT @SoyNovioDeTodas: It's 2016, and a racist, ...\n",
       "5    Worth a read whether you do or don't believe i...\n",
       "6    RT @thenation: Mike Pence doesn’t believe in g...\n",
       "7    RT @makeandmendlife: Six big things we can ALL...\n",
       "8    @AceofSpadesHQ My 8yo nephew is inconsolable. ...\n",
       "9    RT @paigetweedy: no offense… but like… how do ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['message'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_with_no_labels.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall go over several steps to clean the tweets dataset to remove the unnecessary content and highlight the key attributes suitable for the ML model.\n",
    "\n",
    "### Step 1: Punctuation\n",
    "The message text has several punctuations. Punctuations are often unnecessary as it doesn’t add value or meaning to the NLP model. The “string” library has 32 punctuations. The punctuations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the punctuation in our dataset, let’s create a function and apply the function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT RawStory Researchers say we have three year...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TodayinMaker WIRED  2016 was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT SoyNovioDeTodas Its 2016 and a racist sexis...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesnt think carbon dio...   625221\n",
       "1          1  Its not like we lack evidence of anthropogenic...   126103\n",
       "2          2  RT RawStory Researchers say we have three year...   698562\n",
       "3          1  TodayinMaker WIRED  2016 was a pivotal year in...   573736\n",
       "4          1  RT SoyNovioDeTodas Its 2016 and a racist sexis...   466954"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "df_train['message']=df_train['message'].apply(lambda x: remove_punctuation(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "Tokenizing is the process of splitting strings into a list of words. We will make use of Regular Expressions or regex to do the splitting. Regex can be used to describe a search pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re \n",
    "\n",
    "# def tokenize(text):\n",
    "#     split=re.split(\"\\W+\",text) \n",
    "#     return split\n",
    "# df_train['message']=df_train['message'].apply(lambda x: tokenize(x.lower()))\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Stop words\n",
    "\n",
    "Now, we have a list of words without any punctuation. Let’s go ahead and remove the stop words. Stop words are irrelevant words that won’t help in identifying a text as real or fake. We will use “nltk” library for stop-words and some of the stop words in this library are :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The column after step 3 has removed the unnecessary stop words.\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     text=[word for word in text if word not in stopword]\n",
    "#     return text\n",
    "# df_train['message'] = df_train['message'].apply(lambda x: remove_stopwords(x))\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Lemmatize/ Stem\n",
    "\n",
    "Stemming and Lemmatizing is the process of reducing a word to its root form. The main purpose is to reduce variations of the same word, thereby reducing the corpus of words we include in the model. The difference between stemming and lemmatizing is that, stemming chops off the end of the word without taking into consideration the context of the word. Whereas, Lemmatizing considers the context of the word and shortens the word into its root form based on the dictionary definition. Stemming is a faster process compared to Lemmantizing. Hence, it a trade-off between speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Other steps\n",
    "\n",
    "Other cleaning steps can be performed based on the data. I have listed a few of them below,\n",
    "\n",
    "1) Remove URLs\n",
    "2) Remove HTML tags\n",
    "3) Remove emoji\n",
    "4) Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15819, 3)\n",
      "(10546, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['message']\n",
    "y = df_train.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, \n",
    "                 random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the tweets\n",
    "We have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.\n",
    "\n",
    "To do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.\n",
    "\n",
    "Once this is complete, we can start modeling with the new vectorized tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000005', '009barca', '010536', '012', '02', '02cents0', '0519am', '07', '094', '0bamas']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000005', '009barca', '010536', '012', '02', '02cents0', '0519am', '07', '094', '0bamas']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amp  believe  change  climate  doesnt  global  rt  trump  warming\n",
      "0    0        0       2        1       0       0   0      1        0\n",
      "1    0        0       1        1       0       0   0      0        0\n",
      "2    0        0       1        1       0       0   1      0        0\n",
      "3    0        0       0        0       0       1   1      1        1\n",
      "4    0        0       1        1       0       0   0      0        0\n",
      "   000005  009barca  010536  012   02  02cents0  0519am   07  094  0bamas  \\\n",
      "0     0.0       0.0     0.0  0.0  0.0       0.0     0.0  0.0  0.0     0.0   \n",
      "1     0.0       0.0     0.0  0.0  0.0       0.0     0.0  0.0  0.0     0.0   \n",
      "2     0.0       0.0     0.0  0.0  0.0       0.0     0.0  0.0  0.0     0.0   \n",
      "3     0.0       0.0     0.0  0.0  0.0       0.0     0.0  0.0  0.0     0.0   \n",
      "4     0.0       0.0     0.0  0.0  0.0       0.0     0.0  0.0  0.0     0.0   \n",
      "\n",
      "   ...   เล  และ   ได  と通知した  どうなる米国  スタリん時代のソ連や毛沢東の文化大革命並のサイエンスに政治的介入だ  \\\n",
      "0  ...  0.0  0.0  0.0    0.0     0.0                                0.0   \n",
      "1  ...  0.0  0.0  0.0    0.0     0.0                                0.0   \n",
      "2  ...  0.0  0.0  0.0    0.0     0.0                                0.0   \n",
      "3  ...  0.0  0.0  0.0    0.0     0.0                                0.0   \n",
      "4  ...  0.0  0.0  0.0    0.0     0.0                                0.0   \n",
      "\n",
      "   地球温暖化会議  気候変動との表現を削除しないと予算を貰えない  申請書のアブストラクトに  米国政府はある研究者に連絡して  \n",
      "0      0.0                     0.0           0.0              0.0  \n",
      "1      0.0                     0.0           0.0              0.0  \n",
      "2      0.0                     0.0           0.0              0.0  \n",
      "3      0.0                     0.0           0.0              0.0  \n",
      "4      0.0                     0.0           0.0              0.0  \n",
      "\n",
      "[5 rows x 24702 columns]\n",
      "{'change', 'climate'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5364872629764413\n",
      "[[2213  214]\n",
      " [ 779  281]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=[1,2])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6238268530932771\n",
      "[[2794   23]\n",
      " [ 780  408]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = [1,2])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.6843516567707336\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.7013981995786248\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.6855008619038498\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.6736257421949818\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.6613675541084083\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.6506416395326565\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.6441294771116645\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.6381919172572305\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.6357019728021451\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.6265083317372151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed Abubakar\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 [(-10.21383886170312, '000005'), (-10.21383886170312, '009barca'), (-10.21383886170312, '010536'), (-10.21383886170312, '012'), (-10.21383886170312, '02'), (-10.21383886170312, '02cents0'), (-10.21383886170312, '0519am'), (-10.21383886170312, '07'), (-10.21383886170312, '094'), (-10.21383886170312, '0x526978'), (-10.21383886170312, '100000'), (-10.21383886170312, '10000yr'), (-10.21383886170312, '1000s'), (-10.21383886170312, '1001'), (-10.21383886170312, '100daysofshame'), (-10.21383886170312, '100h'), (-10.21383886170312, '100isnow'), (-10.21383886170312, '100s'), (-10.21383886170312, '100th'), (-10.21383886170312, '100x')]\n",
      "0 [(-8.208423438822152, 'noncompetitive'), (-8.197428484680545, 'make'), (-8.179573538953877, 'chinese'), (-8.166554237374992, 'hell'), (-8.159452954792574, 'money'), (-8.131251165939933, 'just'), (-8.116307938134558, 'fake'), (-8.09915477028163, 'manmade'), (-8.069777588427261, 'hoax'), (-7.996223622190371, 'amp'), (-7.984172853819045, 'man'), (-7.909185107947511, 'data'), (-7.843405528567465, 'scam'), (-7.816384864775585, 'real'), (-7.789983833894571, 'science'), (-7.786821078046607, 'stevesgoddard'), (-7.498254583176541, 'realdonaldtrump'), (-6.7657307846980554, 'rt'), (-6.296379134304791, 'warming'), (-6.2876451580382895, 'global')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed Abubakar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', \n",
    "                                   min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create count train and test variables\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create tfidf train and test variables\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a multinomial naive Bayes model\n",
    "Now that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?\n",
    "\n",
    "To assess the accuracies, we will print the test sets accuracy scores for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Tfidf Score:  0.6238268530932771\n",
      "NaiveBayes Count Score:  0.5364872629764413\n"
     ]
    }
   ],
   "source": [
    "# Create a MulitnomialNB model\n",
    "tfidf_nb = MultinomialNB()\n",
    "tfidf_nb.fit(tfidf_train, y_train)\n",
    "\n",
    "# Run predict on your TF-IDF test data to get your predictions\n",
    "tfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "tfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n",
    "\n",
    "# Create a MulitnomialNB model\n",
    "count_nb = MultinomialNB()\n",
    "count_nb.fit(count_train, y_train)\n",
    "\n",
    "# Run predict on your count test data to get your predictions\n",
    "count_nb_pred = count_nb.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "count_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n",
    "\n",
    "print('NaiveBayes Tfidf Score: ', tfidf_nb_score)\n",
    "print('NaiveBayes Count Score: ', count_nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out another classifier: Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Score:   0.725\n",
      "[[2445  212]\n",
      " [ 305  842]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_svc = LinearSVC()\n",
    "tfidf_svc.fit(tfidf_train, y_train)\n",
    "tfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n",
    "tfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n",
    "\n",
    "print(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n",
    "\n",
    "svc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred, labels=[1, 2])\n",
    "print(svc_cm)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.87      0.80      2817\n",
      "           2       0.72      0.71      0.72      1188\n",
      "\n",
      "   micro avg       0.74      0.82      0.78      4005\n",
      "   macro avg       0.74      0.79      0.76      4005\n",
      "weighted avg       0.74      0.82      0.78      4005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, tfidf_svc_pred, labels=[1, 2]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a46c5a272d30d69d5c3259eb2e583983792729bf3d55d9dc6d8feb501fa0ad45"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
